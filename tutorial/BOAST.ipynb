{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BOAST Interactive Advanced Tutorial\n",
    "## Symmetric Result DGEMM inner kernel\n",
    "Based on the work of Eric Bainville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Load BOAST and bring BOAST namespace into the global namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "require 'BOAST'\n",
    "include BOAST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wil first Be importing the reference implementation of the kernel directly from BigDFT sources (slight modification to remove the accumulation in the result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_lang(C)\n",
    "set_array_start(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_map = { 4 => NArray::SFLOAT, 8 => NArray::FLOAT}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This reference kernel computes a 2x4 block (transposed) of a matrix multiply operation C = A * B.\n",
    "As acccesses can be amortized for big matrix (n^2 acesses n^3 computations) the lines of A and collumns of B are first interleaved. This kernel expects 2 interleaved lines of A and 4 interleaved columns of B.\n",
    "\n",
    " - packed lines: $A[0]$, $A[1]$, $A[LDA]$, $A[LDA+1]$, $A[2]$, $A[3]$, $A[LDA+2]$, $A[LDA+3]$\n",
    " - packed collumns: $B[0]$, $B[1]$, $B[LDB]$, $B[LDB+1]$, $B[2*LDB]$, $B[2*LDB+1]$, $B[3*LDB]$, $B[3*LDB+1]$, $B[2]$, $B[3]$, $B[LDB+2]$, $B[LDB+3]$, etc...\n",
    " \n",
    "The kernel is unrolled by a factor of 4. Maintaining and upgrading such a kernel is costly and a lot of hand tuning took place to obtain this version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reference\n",
    "  tsy = 2\n",
    "  tsx = 4\n",
    "  vl = 2\n",
    "  alignment = vl*get_default_real_size\n",
    "  # We describe the parameters of the procedure so that BOAST can interface with it.\n",
    "  nvec = Int :nvec, dir: :in\n",
    "  a = Real :a, dim: [Dim(vl), Dim(tsy), Dim(nvec)], dir: :in\n",
    "  b = Real :b, dim: [Dim(vl), Dim(tsx), Dim(nvec)], dir: :in\n",
    "  c = Real :c, dim: [Dim(tsy), Dim(tsx)], dir: :inout\n",
    "  ldc = Int :ldc, dir: :in\n",
    "  p = Procedure( :gemm_block_2x4_c, [a, b, nvec, c, ldc])\n",
    "  # The source of the kernel is just a string containing the original procedure.\n",
    "  # The kernel is in C and needs a header in order to compile.\n",
    "  k = CKernel::new(lang: C, includes: \"immintrin.h\") {\n",
    "  get_output.puts <<EOF\n",
    "void gemm_block_2x4_c(const double * a,const double * b,long long int n,double * y,long long int ldy)\n",
    "{\n",
    "  __m128d A0,A1,B0,B1,B2,B3;\n",
    "  __m128d S00,S01,S02,S03,S10,S11,S12,S13;\n",
    "  S00 = _mm_setzero_pd();\n",
    "  S01 = _mm_setzero_pd();\n",
    "  S02 = _mm_setzero_pd();\n",
    "  S03 = _mm_setzero_pd();\n",
    "  S10 = _mm_setzero_pd();\n",
    "  S11 = _mm_setzero_pd();\n",
    "  S12 = _mm_setzero_pd();\n",
    "  S13 = _mm_setzero_pd();\n",
    "  unsigned long long int k=n>>2;\n",
    "\n",
    "  do {\n",
    "    A0 = _mm_load_pd(a);\n",
    "    a += 4;\n",
    "    B0 = _mm_load_pd(b);\n",
    "    S00 = _mm_add_pd(S00,_mm_mul_pd(A0,B0));\n",
    "    B1 = _mm_load_pd(b+2);\n",
    "    b += 8;\n",
    "    S01 = _mm_add_pd(S01,_mm_mul_pd(A0,B1));\n",
    "    B2 = _mm_load_pd(b-4);\n",
    "    S02 = _mm_add_pd(S02,_mm_mul_pd(A0,B2));\n",
    "    B3 = _mm_load_pd(b-2);\n",
    "    S03 = _mm_add_pd(S03,_mm_mul_pd(A0,B3));\n",
    "    A1 = _mm_load_pd(a-2);\n",
    "    S10 = _mm_add_pd(S10,_mm_mul_pd(A1,B0));\n",
    "    S11 = _mm_add_pd(S11,_mm_mul_pd(A1,B1));\n",
    "    S12 = _mm_add_pd(S12,_mm_mul_pd(A1,B2));\n",
    "    S13 = _mm_add_pd(S13,_mm_mul_pd(A1,B3));\n",
    "    A0 = _mm_load_pd(a);\n",
    "    a += 4;\n",
    "    B0 = _mm_load_pd(b);\n",
    "    S00 = _mm_add_pd(S00,_mm_mul_pd(A0,B0));\n",
    "    B1 = _mm_load_pd(b+2);\n",
    "    b += 8;\n",
    "    S01 = _mm_add_pd(S01,_mm_mul_pd(A0,B1));\n",
    "    B2 = _mm_load_pd(b-4);\n",
    "    S02 = _mm_add_pd(S02,_mm_mul_pd(A0,B2));\n",
    "    B3 = _mm_load_pd(b-2);\n",
    "    S03 = _mm_add_pd(S03,_mm_mul_pd(A0,B3));\n",
    "    A1 = _mm_load_pd(a-2);\n",
    "    S10 = _mm_add_pd(S10,_mm_mul_pd(A1,B0));\n",
    "    S11 = _mm_add_pd(S11,_mm_mul_pd(A1,B1));\n",
    "    S12 = _mm_add_pd(S12,_mm_mul_pd(A1,B2));\n",
    "    S13 = _mm_add_pd(S13,_mm_mul_pd(A1,B3));\n",
    "    A0 = _mm_load_pd(a);\n",
    "    a += 4;\n",
    "    B0 = _mm_load_pd(b);\n",
    "    S00 = _mm_add_pd(S00,_mm_mul_pd(A0,B0));\n",
    "    B1 = _mm_load_pd(b+2);\n",
    "    b += 8;\n",
    "    S01 = _mm_add_pd(S01,_mm_mul_pd(A0,B1));\n",
    "    B2 = _mm_load_pd(b-4);\n",
    "    S02 = _mm_add_pd(S02,_mm_mul_pd(A0,B2));\n",
    "    B3 = _mm_load_pd(b-2);\n",
    "    S03 = _mm_add_pd(S03,_mm_mul_pd(A0,B3));\n",
    "    A1 = _mm_load_pd(a-2);\n",
    "    S10 = _mm_add_pd(S10,_mm_mul_pd(A1,B0));\n",
    "    S11 = _mm_add_pd(S11,_mm_mul_pd(A1,B1));\n",
    "    S12 = _mm_add_pd(S12,_mm_mul_pd(A1,B2));\n",
    "    S13 = _mm_add_pd(S13,_mm_mul_pd(A1,B3));\n",
    "    A0 = _mm_load_pd(a);\n",
    "    a += 4;\n",
    "    B0 = _mm_load_pd(b);\n",
    "    S00 = _mm_add_pd(S00,_mm_mul_pd(A0,B0));\n",
    "    B1 = _mm_load_pd(b+2);\n",
    "    b += 8;\n",
    "    S01 = _mm_add_pd(S01,_mm_mul_pd(A0,B1));\n",
    "    B2 = _mm_load_pd(b-4);\n",
    "    S02 = _mm_add_pd(S02,_mm_mul_pd(A0,B2));\n",
    "    B3 = _mm_load_pd(b-2);\n",
    "    S03 = _mm_add_pd(S03,_mm_mul_pd(A0,B3));\n",
    "    A1 = _mm_load_pd(a-2);\n",
    "    S10 = _mm_add_pd(S10,_mm_mul_pd(A1,B0));\n",
    "    S11 = _mm_add_pd(S11,_mm_mul_pd(A1,B1));\n",
    "    S12 = _mm_add_pd(S12,_mm_mul_pd(A1,B2));\n",
    "    S13 = _mm_add_pd(S13,_mm_mul_pd(A1,B3));\n",
    "  } while (--k>0);\n",
    "  _mm_store_pd(y, _mm_hadd_pd(S00,S10));\n",
    "  _mm_store_pd(y+ldy, _mm_hadd_pd(S01,S11));\n",
    "  _mm_store_pd(y+2*ldy, _mm_hadd_pd(S02,S12));\n",
    "  _mm_store_pd(y+3*ldy, _mm_hadd_pd(S03,S13));\n",
    "}\n",
    "EOF\n",
    "  }\n",
    "  # we set the entry point of the kernel\n",
    "  k.procedure = p\n",
    "  k\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These helper function pack lines and collumns from the source matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_lines(mat, line_start, tile_size_y:, vector_length:, unroll:, **ignored)\n",
    "  n_vec = (mat.shape[0].to_f / vector_length).ceil\n",
    "  n_vec = (n_vec.to_f/unroll).ceil*unroll\n",
    "  package = ANArray::new(mat.typecode, vector_length*mat.element_size,\n",
    "    vector_length, tile_size_y, n_vec)\n",
    "  mat.shape[0].times { |i|\n",
    "    (line_start...[line_start + tile_size_y, mat.shape[1]].min).each { |j|\n",
    "      package[i%vector_length, j-line_start, i/vector_length] = mat[i, j]\n",
    "    }\n",
    "  }\n",
    "  package\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_columns(mat, column_start, tile_size_x:, vector_length:, unroll:, **ignored)\n",
    "  n_vec = (mat.shape[1].to_f / vector_length).ceil\n",
    "  n_vec = (n_vec.to_f/unroll).ceil*unroll\n",
    "  package = ANArray::new(mat.typecode, vector_length*mat.element_size,\n",
    "    vector_length, tile_size_x, n_vec)\n",
    "  mat.shape[1].times { |i|\n",
    "    (column_start...[column_start + tile_size_x, mat.shape[0]].min).each { |j|\n",
    "      package[i%vector_length, j-column_start, i/vector_length] = mat[j, i]\n",
    "    }\n",
    "  }\n",
    "  package\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the reference\n",
    "We will be using NArray matricx multiply implementation to generate a result block of the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = NMatrix::new( type_map[get_default_real_size], 5000, 100 ).randomn!\n",
    "b = a.transpose\n",
    "c = a * b\n",
    "column_start = 8\n",
    "line_start = 4\n",
    "tsy = 2\n",
    "tsx = 4\n",
    "p c_ruby_block = c[line_start...(line_start+tsy), column_start...(column_start+tsx)]\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pack the corresponding lines and columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = pack_lines(a, 4, vector_length: 2, tile_size_x: 4, tile_size_y:2, unroll: 4)\n",
    "pc = pack_columns(b, 8, vector_length: 2, tile_size_x: 4, tile_size_y:2, unroll: 4)\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the computing kernel and build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_ref = reference\n",
    "k_ref.build\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We allocate a result buffer and set some performance evaluation variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_ref_block = NMatrix::new( type_map[get_default_real_size], 2, 4 )\n",
    "repeat = 100\n",
    "repeat_inner = 10\n",
    "epsilon = 1e-8\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We execute the kernel $repeat$ times and gather the result. Inside of boast the computation is done $repeat_inner$ times to account for the very short computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = repeat.times.collect {\n",
    "  k_ref.run(pl, pc, pl.shape[2], c_ref_block, 2, repeat: repeat_inner)\n",
    "}\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We verify the precision of the computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p c_ref_block\n",
    "err = c_ruby_block - c_ref_block\n",
    "raise \"Computation error\" if err.abs.max > epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pick the shortest runitme to compute the performance of the kernel. Arithmetic complexity is $2 * ALineLength * TileSizex *TileSizey$. And we account for the repetition inside of BOAST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = res.min { |r1, r2|\n",
    "  r1[:duration] <=> r2[:duration]\n",
    "}\n",
    "perf = repeat_inner * 2 * a.shape[0] * 4 * 2 / (best[:duration] * 1e9 )\n",
    "puts \"time: #{best[:duration]} s, GFlops: #{perf}\"\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new Meta-Programmed Implementation\n",
    "We want to be able to chose the length of the vectors, the tiling and the inner unrolling. So we define a function that accpets those parameters and that will return the corresponding kernel. For default values we use the parameters from the reference kernel. We slightly modified the interface so that the kernel also compiles using FORTRAN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inner_kernel(opts={})\n",
    "  default = {vector_length: 2, tile_size_x: 4, tile_size_y:2, unroll: 4}\n",
    "  opts = default.merge(opts)\n",
    "  vl = opts[:vector_length]\n",
    "  tsx = opts[:tile_size_x]\n",
    "  tsy = opts[:tile_size_y]\n",
    "  unroll = opts[:unroll]\n",
    "\n",
    "  # We describe the interface of the procedure:\n",
    "  nvec = Int :n, dir: :in\n",
    "  a = Real :a, vector_length: vl, dim: [Dim(tsy), Dim(nvec)], dir: :in, restrict: true\n",
    "  b = Real :b, vector_length: vl, dim: [Dim(tsx), Dim(nvec)], dir: :in, restrict: true\n",
    "  c = Real :c, dim: [Dim(tsy),Dim(tsx)], dir: :inout\n",
    "  # The procedure body directly follows the Procedure declaration:\n",
    "  p = Procedure( :\"inner_v#{vl}_x#{tsx}_y#{tsy}_u#{unroll}\", [nvec, a, b, c]) {\n",
    "    # We allocate one vector per resulting value:\n",
    "    tmp_res = tsx.times.collect { |k|\n",
    "      tsy.times.collect { |l|\n",
    "        Real :\"tmpres_#{k}_#{l}\", vector_length: vl\n",
    "      }\n",
    "    }\n",
    "    # And one temporary value per tile dimension:\n",
    "    tmp_a = tsy.times.collect { |l|\n",
    "      Real :\"tmpa_#{l}\", vector_length: vl\n",
    "    }\n",
    "    tmp_b = tsx.times.collect { |l|\n",
    "      Real :\"tmpb_#{l}\", vector_length: vl\n",
    "    }\n",
    "    # An iterator variable\n",
    "    i = Int :i\n",
    "\n",
    "    # Declaration of all the variables\n",
    "    decl *tmp_res.flatten\n",
    "    decl *tmp_a\n",
    "    decl *tmp_b\n",
    "    decl i\n",
    "    \n",
    "    # Initialization of the temporary result values\n",
    "    tmp_res.flatten.each { |tmp|\n",
    "      pr tmp.set 0.0\n",
    "    }\n",
    "    # The inner block will multiply tsy vectors of A with tsx vectors of B\n",
    "    # We use a flag map to keep track of the values that have already been loaded\n",
    "    p_inn = lambda { |offset|\n",
    "      loaded = {}\n",
    "      tsy.times { |k|\n",
    "        pr tmp_a[k] === a[k, offset]\n",
    "        tsx.times { |l|\n",
    "          unless loaded[l]\n",
    "            pr tmp_b[l] === b[l, offset]\n",
    "            loaded[l] = true\n",
    "          end\n",
    "          # The FMA(a, b, c) function will return a * b + c\n",
    "          # if the architecture doesn't support FMA\n",
    "          pr tmp_res[l][k] === FMA(tmp_a[k], tmp_b[l], tmp_res[l][k])\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "    # The main loop, unrolled\n",
    "    pr For(i, 0, nvec - 1, step: unroll) {\n",
    "      unroll.times { |j|\n",
    "        p_inn.call(i+j)\n",
    "      }\n",
    "    }\n",
    "    # We compute the resulting values by reducing the vectors components\n",
    "    tsy.times { |k|\n",
    "      tsx.times { |j|\n",
    "        pr c[k,j] === vl.times.collect { |l| tmp_res[j][k][l] }.reduce(:+)\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # The Kernel body is the BOAST print of the procedure so we can use\n",
    "  p.ckernel(:includes => \"immintrin.h\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = inner_kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set_verbose(true)\n",
    "#set_debug_source(true)\n",
    "k.build\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking the kernel result and first performance evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_block = NMatrix::new( type_map[get_default_real_size], 2, 4)\n",
    "res = repeat.times.collect {\n",
    "  k.run(pl.shape[2], pl, pc, c_block, repeat: repeat_inner)\n",
    "}\n",
    "p c_block\n",
    "err = c_ruby_block - c_block\n",
    "raise \"Computation error\" if err.abs.max > epsilon\n",
    "best = res.min { |r1, r2|\n",
    "  r1[:duration] <=> r2[:duration]\n",
    "}\n",
    "perf = repeat_inner * 2 * a.shape[0] * 4 * 2 / (best[:duration] * 1e9 )\n",
    "puts \"time: #{best[:duration]} s, GFlops: #{perf}\"\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating new versions, checking the correctness and finding the best version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firs define the exploration space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = OptimizationSpace::new(vector_length: [2,4], tile_size_x: 1..4, tile_size_y: 1..4, unroll: 1..4)\n",
    "s.to_h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer will create all possible configuration and pass them to the block. The result of the block will be the metric used and the optimizer will search the configuration yielding the minimum metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimum = BruteForceOptimizer::new(s).optimize { |opts|\n",
    "  p opts\n",
    "  tsx = opts[:tile_size_x]\n",
    "  tsy = opts[:tile_size_y]\n",
    "  c_ruby_block = c[line_start...(line_start + tsy), column_start...(column_start + tsx)]\n",
    "  # We create an compile a new kernel for each configuration\n",
    "  k = inner_kernel(**opts)\n",
    "  k.build\n",
    "  #Instanciate a new result block and pack the corresponding lines and columns\n",
    "  c_block = NMatrix::new( type_map[get_default_real_size], tsy, tsx)\n",
    "  pl = pack_lines(a, line_start, **opts)\n",
    "  pc = pack_columns(b, column_start, **opts)\n",
    "  # We evaluate the kernel's performance and accuracy using the same method as before\n",
    "  res = repeat.times.collect {\n",
    "    k.run(pl.shape[2], pl, pc, c_block, repeat: repeat_inner)\n",
    "  }\n",
    "  err = c_ruby_block - c_block\n",
    "  err.each { |v|\n",
    "    if v.abs > epsilon\n",
    "      puts k\n",
    "      puts \"c:\"\n",
    "      p c_ruby_block\n",
    "      puts \"c_block:\"\n",
    "      p c_block\n",
    "      puts \"error:\"\n",
    "      p err\n",
    "      raise \"Computation error!\" if v.abs > epsilon\n",
    "    end\n",
    "  }\n",
    "  best = res.min { |r1, r2|\n",
    "    r1[:duration] <=> r2[:duration]\n",
    "  }\n",
    "  perf = repeat_inner * 2 * a.shape[0] * tsx * tsy / (best[:duration] * 1e9 )\n",
    "  puts \"time: #{best[:duration]} s, GFlops: #{perf}\"\n",
    "  # we try to maximize perf so we return the inverse\n",
    "  1.0/perf\n",
    "}\n",
    "nil;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting configuration, using the default compilation flags and the experimenters architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p optimum[0]\n",
    "puts \"GFlops: #{1/optimum[1]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = inner_kernel(**optimum[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Ruby 2.5.1",
   "language": "ruby",
   "name": "ruby"
  },
  "language_info": {
   "file_extension": ".rb",
   "mimetype": "application/x-ruby",
   "name": "ruby",
   "version": "2.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
